---
layout: post
title: "Designing a Feminist Alexa"
date: 2018-10-19 18:00:00
teacher: "Various"
unit: "UAL Creative Computing Institute"
abstract: ""
website: ""
thumb: ""
---

This is the first public event of [the creative computing institute](https://www.arts.ac.uk/creative-computing-institute)

**Feminist Internet** are the first research group. This comes out of ual futures. The mission is to advance equality on the internet.

## Gendering of personal assistants (i.e Alexa, Google, Siri, Cortana)

The internet of things is a thing.

(Why is every slide a gif)

These personal assistants have two components:

- Networked objects, which are a part of everyday life
- Intelligent algorithms which are making decisions all around us

Buolamwini (2018):

> We have entered the age of automation overconfident, yet underprepared. If we fail to make ethical and inclusive artificial intelligence we risk losing gains made in civil rights and gender equity under the guise of machine neutrality.

[Source](http://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212)

Tae (the Microsoft chatbot)

> The more you chat with Tay the smarter she gets, so the experience can be more personalized to you

She turned into a nazi blah blah we know the story

[Microsoft Zo is the latest iteration](https://www.zo.ai/) of this.

People working on AI ethics:

- [Ada Lovelace Institute](https://www.adalovelaceinstitute.org/)
- [Centre for Data Ethics and Innovation](https://www.gov.uk/government/consultations/consultation-on-the-centre-for-data-ethics-and-innovation/centre-for-data-ethics-and-innovation-consultation)
- [Women in AI](http://womeninai.co/)
- [AI Now Institute](https://ainowinstitute.org/)

_You get a think tank! And you get a think tank!_

Jaqline Feldman (2016) in the [New Yorker](https://www.newyorker.com/tech/annals-of-technology/the-bot-politic):

> By creating interactions that encourage consumers to understand the objects that serve them as women, technologists abet the prejudice by which women are considered objects.

Tech companies say to this: It's just what the market wants. AIs are designed as women, respond to abusive language in ways that reinforce stereotypes.

Leah Fessler, Quartz Magazine (2017): [We tested bots like Siri and Alexa to see who would stand up to sexual harassment](https://qz.com/911681/we-tested-apples-siri-amazon-echos-alexa-microsofts-cortana-and-googles-google-home-to-see-which-personal-assistant-bots-stand-up-for-themselves-in-the-face-of-sexual-harassment/)

There's been some pushback to that (and some changes), but that's not as good as intervening at the design/development stage.

## Panel

- Feminist AI researcher **Josie Young**
- Founder Acorn Aspirations, Teens in AI **Elena Sinel** (_Yuch a business person_)
- Co-Founder, Head Creative Technologist, [Comuzi](https://comuzi.xyz/) **Alex Fefegha** (Comuzi is an ad agency)

## Josie Young on Feminist Chatbots

How do we interrogate how we design chatbots? Chatbots are probably the main interface we have with AI (citation needed). If you call up a government agency, talk to your laptop, Facebook etc., you're taling to a chatbot. Biases in chatbots _seep back_ into society in all kinds of ways.

Should chatbots have a gender? Nope.

- When a gender is attached to chatbots, it's usually done in a stereotypical way. Female bots are assistants, navigation, male bots gove law advice etc.
- When we gender bots, it prompts negative reactions from people interacting with it. When Cortana was first deployed, the most common question was wether she had a boyfriend (citation needed).
- This tech reaches a lot of people, so there's a huge amount of responsibility
- Constrains design of robots - you've limited how that bot can express itself, connect with others, what it can do.

### Feminist research design process

Uzbekistan isn't a great place.

[Teens in AI](https://teensinai.com/) does bootcamps, hackathons etc.

{% include embed.liquid, src: "https://www.youtube.com/embed/61zXhXcf6Vw", format: 56.25%}

ehhhhh.

## Alex Fefegha: Algorithms and the Life of Brisha Borden

This is his CSM MA Thesis.

AI as

> The study of how to make computers do things at which, at the moment, people are better
> [Rich and Knight (1991)](https://people.eecs.berkeley.edu/~russell/intro.html)

Brisha Borden was a Florida teen. She got arrested for stealing a bike - the judge in the case was using a re-offending score software. Of course the thing's racist.

This is detailed in a [2016 ProPublica Investigation](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)

The offenders in Florida would get a survey where they ask questions which are essentially designed to filter out poor people. Of course this plays into disproportionate sentencing of black people in the US>

Responses to the ProPublica piece:

- [False Positives, False Negatives, and False Analyses: A Rejoinder to "Machine Bias: There's Software Used Across the Country to Predict Future Criminals. And It's Biased Against Blacks.](http://www.uscourts.gov/federal-probation-journal/2016/09/false-positives-false-negatives-and-false-analyses-rejoinder)
- [COMPAS Risk Scales: Demonstrating](http://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf)
- [the accuracy, fairness, and limits of predicting recidivism](http://advances.sciencemag.org/content/4/1/eaao5580)

- All of this software is written by private companies, the algorithms aren't transparent.
- Based on group data, but making decisions on individuals. _But could you argue that we make all kinds of decisions in this way, i.e public health questions_
- Question of national v.s. local data

This is all based on US data and reporting, how does this play in a UK context. Ran workshops etc. with Comuzi.

### Conclusions

- AI is just making bad decisions faster
- Fairness is a subjective thing, hard to do with maths (_That seems like a very broad statement_)
- Accountability of algorithms and data is hard
- More conversations on bias are needed
- Teams need to be diverse

Philip Alston:

> It is extremely important for an audience interested in AI to recognize that when we take a social welfare system and ... put on top of it ways to make it more efficient, what weâ€™re doing is doubling down on injustices

[Source](https://www.fastcompany.com/90252753/a-skeptics-guide-to-thinking-about-ai)

[AI Cheatsheet](https://aicheatsheet.comuzi.xyz/)

## Questions

### How do we balance changing tech vs changing society

- **Young**: They need to be intertwined. If we bring in social scientists, philosophers together with people building the AI this can happen.
- **Fefegha**: Tech is an extension of ourselves. Humanity isn't nice, but if we're going to introduce AI systems we need to recognize our on flaws. We need to have conversations on how data collection is biased etc.
- **Young**: Governments and companies are setting the classification, who goes in a residual category etc.

### Do we need standards / global regulation for AI

- **Fefegha**: Part of [IEEE](https://www.ieee.org/), which is trying to develop standards for building ethical AI. We can build these frameworks, but how are they enforced, measured, regulated. Of course industry is trying to avoid regulation.
- **Young**: This all needs to happen at different levels / layers: The teams building the software, people using it, governments regulating it etc. Also: Innoation used to happen in universities (which have all this ethics infrastructure) - now that happens inside companies, which don't have any of those frameworks.

### Calvert makes her point about artificial intelligence v.s. partial intelligence

**Fefegha**: I stay away from that conversation and focus on real-world issues that affect people now (i.e sentencing)
**Young**: A more opimistic of the future, where AI creates, works together. [Her (2013)](https://www.imdb.com/title/tt1798709/) as opposed to [Ex Machina (2014)](https://www.imdb.com/title/tt0470752/).
